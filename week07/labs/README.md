## DL 201 LAB **Under Revision**
  
Today we have a lab from our guest speaker [Chahhou Mohammed](https://docs.google.com/presentation/d/e/2PACX-1vRFn2idmx2dIu0Q2cfQaTKsPqdVX3rO2Yb9JTnm2jFX1BnEdvdVBAVmBLRzRE5y05m0ALhwMvjZ3mTg/pub?start=true&loop=false&delayms=3000). 

If you do not have a kaggle user, please [sign up](https://www.kaggle.com/).  
  
Go to the [quora competition data page](https://www.kaggle.com/c/quora-insincere-questions-classification/data) and click on one of the data downloads, a pop up will appear to accept the rules.
  
Click on book (this book)[tbd], and press `Fork` in the top right corner. This will create a book under your own user. 

#### Demo Notebooks
https://www.kaggle.com/mchahhou/part1-optimizers
https://www.kaggle.com/mchahhou/part2-transfer-learning
https://www.kaggle.com/mchahhou/part3-preprocessing-and-finetuning
https://www.kaggle.com/mchahhou/part4-meta-embeddings
https://www.kaggle.com/mchahhou/part5-lab

#### Lab 
**Make a submsission to Kaggle with your prediction.**
 
*Hints to improve the score* 
* We were using only 50% of the dataset to train our NN.
* The NN architecture can be improved.
* We didnt use any regularization (L1/L2, dropout, SpatialDropout)
* Improve the text preprocessing
* Try other embeddings - FastText Crawl, Glove, etc. 
* Try other finetuning approaches (another approach that can be used is setting a different LR per layer)
* Increase the size of bagging
* Ensemble different architectures
* Read kernels from https://www.kaggle.com/c/quora-insincere-questions-classification/kernels and try other ideas
