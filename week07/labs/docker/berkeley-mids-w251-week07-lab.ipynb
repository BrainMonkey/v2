{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References :    \n",
    "    [rxrx.ai](http://rxrx.ai)    \n",
    "    https://www.kaggle.com/jesucristo/quick-visualization-eda    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:Papyrus; font-size:2em;\">Recursion Cellular Image Classification</span>\n",
    "# <span style=\"font-family:Papyrus; font-size:1em;\">CellSignal: Disentangling biological signal from experimental noise in cellular images</span>\n",
    "\n",
    "![](https://assets.website-files.com/5cb63fe47eb5472014c3dae6/5d040176f0a2fd66df939c51_figure1%400.75x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:Papyrus; font-size:1em;\">Experiment Structure</span>\n",
    "\n",
    "![](https://assets.website-files.com/5cb63fe47eb5472014c3dae6/5d03fe719710ab238b5b41b3_384%20Well%402x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# <span style=\"font-family:Papyrus; font-size:1em;\">Images of two different genetic conditions (rows) in HUVEC cells across four experimental batches (columns)</span>\n",
    "\n",
    "![](https://assets.website-files.com/5cb63fe47eb5472014c3dae6/5d040176beb559547adf9464_figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:Papyrus; font-size:1em;\">Each well site has 6-channel fluorescent microscopy images</span>\n",
    "\n",
    "![](https://www.kaggleusercontent.com/kf/16548690/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..UnqOr4CpUjW-5miqdOOSVw.gLsqtIjKR6KozOuYs83v1M_6M5mb1gqwC03CSn9-OAfOEXV6ernPyuMqXpnM6r-M_0iIfIqFU4PWG-Lze_0vfHAMuuQL-MuHdd3ULQNL0Cx7pHR-inpJOaK2XPQ1tJm7VYQhfW4YVdEZrua9S4YLLA.NRRRexPYHU8k3wmAsr1MVQ/__results___files/__results___10_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us begin!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data=\"/data/kaggle/recursion-cellular-image-classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lahtr $path_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_data = '../input/recursion-cellular-image-classification/'\n",
    "trnalldf = pd.read_csv(os.path.join(path_data, 'train.csv'))\n",
    "tstdf = pd.read_csv(os.path.join(path_data, 'test.csv'))\n",
    "statsdf = pd.read_csv(os.path.join(path_data, 'pixel_stats.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets view the file contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnalldf.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets look at the samples per per experiments\n",
    "tstdf.experiment.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take the first 3 experiments as train, and the next 4 as val\n",
    "valdf = trnalldf[trnalldf.experiment.str.contains('01')]\n",
    "trndf = trnalldf[trnalldf.experiment.str.contains('02|03|04|05|06|07')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train frame shape : rows {} cols {}'.format(*trndf.shape))\n",
    "print('Val frame shape : rows {} cols {}'.format(*valdf.shape))\n",
    "print('Test frame shape : rows {} cols {}'.format(*tstdf.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at experiment batch impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsdf.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanexpdf = statsdf.groupby(['experiment', 'channel'])['mean'].mean().unstack()\n",
    "stdexpdf = statsdf.groupby(['experiment', 'channel'])['mean'].mean().unstack()\n",
    "meanexpdf[meanexpdf.index.str.contains('01|02')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see large differences by  \n",
    "meanexpdf.loc['HEPG2-01'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will bring in some augmentations from Albumentations - check it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:Papyrus; font-size:1em;\">Albumentations</span>\n",
    "\n",
    "![](https://camo.githubusercontent.com/041633dc5d522d6cf583a81d4a1d85be87f44155/68747470733a2f2f686162726173746f726167652e6f72672f776562742f652d2f366b2f7a2d2f652d366b7a2d66756770326865616b336a7a6e733362632d72386f2e6a706567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (Cutout, Compose, Normalize, RandomRotate90, HorizontalFlip,\n",
    "                           VerticalFlip, ShiftScaleRotate, Transpose, OneOf, IAAAdditiveGaussianNoise,\n",
    "                           GaussNoise, RandomGamma, RandomContrast, RandomBrightness, HueSaturationValue,\n",
    "                           RandomCrop, Lambda, NoOp, CenterCrop, Resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(p=1.):\n",
    "    return Compose([\n",
    "        RandomRotate90(),\n",
    "        HorizontalFlip(),\n",
    "        VerticalFlip(),\n",
    "        Transpose(),\n",
    "        NoOp(),\n",
    "    ], p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader - we make a class and use torch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDS(D.Dataset):\n",
    "    def __init__(self, df, img_dir, size = 256, mode='train', meandf = meanexpdf, stddf = stdexpdf, channels=[1,2,3,4,5,6]):\n",
    "        \n",
    "        self.records = df.to_records(index=False)\n",
    "        self.channels = channels\n",
    "        self.site = random.randint(1,2) # load a random site from each well.\n",
    "        self.mode = mode\n",
    "        self.meandf = meanexpdf\n",
    "        self.stddf = stdexpdf\n",
    "        self.img_dir = img_dir\n",
    "        self.len = df.shape[0]\n",
    "        self.size = size\n",
    "        self.augtransform = aug()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_img_as_tensor(file_name, size):\n",
    "        with Image.open(file_name) as img:\n",
    "            img = img.resize((size, size), resample=Image.BICUBIC)\n",
    "            return img\n",
    "        \n",
    "    @staticmethod\n",
    "    def torch_augment(img, transform, mean_, sd_):\n",
    "        img = img.astype(np.float32)\n",
    "        img = transform(image = img)['image']\n",
    "        img = torch.from_numpy(np.moveaxis(img, -1, 0).astype(np.float32))\n",
    "        img = T.Normalize([*list(mean_)], [*list(sd_)])(img)\n",
    "        return img  \n",
    "\n",
    "    def _get_img_path(self, index, channel):\n",
    "        experiment, well, plate = self.records[index].experiment, self.records[index].well, self.records[index].plate\n",
    "        return '/'.join([self.img_dir,self.mode,experiment,f'Plate{plate}',f'{well}_s{self.site}_w{channel}.png'])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        paths = [self._get_img_path(index, ch) for ch in self.channels]\n",
    "        \n",
    "        # Normalise values\n",
    "        meanvals = self.meandf.loc[self.records[index].experiment].values\n",
    "        stdvals = self.stddf.loc[self.records[index].experiment].values\n",
    "        \n",
    "        # Load image\n",
    "        img = np.stack([self._load_img_as_tensor(img_path, self.size) for (img_path, m, s) in zip(paths, meanvals, stdvals)], -1)\n",
    "        img = self.torch_augment(img, self.augtransform, meanvals, stdvals)      \n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            return img, self.records[index].sirna\n",
    "        else:\n",
    "            return img, self.records[index].id_code\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnloader = D.DataLoader(ImagesDS(trndf, path_data, mode='train'), batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "valloader = D.DataLoader(ImagesDS(valdf, path_data, mode='train'), batch_size=batch_size*4, shuffle=False, num_workers=8)\n",
    "tstloader = D.DataLoader(ImagesDS(tstdf, path_data, mode='test'), batch_size=batch_size*4, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(iter(trnloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.mean(), X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Batch Shape : {}'.format(X.shape))\n",
    "print('Label Shape : {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's set up our model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# <span style=\"font-family:Papyrus; font-size:1em;\">Densenet121</span>\n",
    "\n",
    "![](https://d3i71xaburhd42.cloudfront.net/501d99e392783e4acafb220136d32ea68a921282/1-Figure1-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000, num_channels=6):\n",
    "        super().__init__()\n",
    "        preloaded = torchvision.models.densenet121(pretrained=True)\n",
    "        self.features = preloaded.features\n",
    "        self.features.conv0 = nn.Conv2d(num_channels, 64, 7, 2, 3)\n",
    "        self.classifier = nn.Linear(1024, num_classes, bias=True)\n",
    "        del preloaded\n",
    "        \n",
    "    def forward(self, x, emb=False):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
    "        if emb:\n",
    "            return out\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(num_classes=trndf.sirna.max()+1, num_channels = 6)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 10 layers\n",
    "[(n, w.shape) for t, (n,w) in enumerate(model.named_parameters()) if t <10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look at a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features.denseblock2.denselayer10.conv2.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision Training\n",
    "We saw apex in homework 6. This allows certain parts of the network to be stored in FP32 (32-bit floating point) and other parts to be stored in FP16\n",
    "With a few small code changes we can half runtime... essential in the large networks outperforming today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# install NVIDIA Apex if needed to support mixed precision training\n",
    "use_amp = True\n",
    "if use_amp:\n",
    "    try:\n",
    "        from apex import amp\n",
    "    except ImportError:\n",
    "        !pip install  -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ../input/*/*/NVIDIA-apex*\n",
    "        from apex import amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use one cycle to get optimum LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model = DenseNet(num_classes=trndf.sirna.max()+1, num_channels = 6)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\", keep_batchnorm_fp32=False, loss_scale=\"dynamic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [One Cycle Policy](https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One cycle policy https://sgugger.github.io/the-1cycle-policy.html#the-1cycle-policy\n",
    "optimizer.param_groups[0]['lr'] = .001 \n",
    "onecycdf = trnalldf\n",
    "onecycleloader = D.DataLoader(ImagesDS(onecycdf, path_data, mode='train'), batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "print('Total Step Count : {}'.format(len(onecycleloader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "init_value = 1e-5\n",
    "final_value=1.\n",
    "beta = 0.98\n",
    "avg_loss = 0.\n",
    "batch_num = 0\n",
    "numsteps = len(onecycleloader)-1\n",
    "mult = (final_value / init_value) ** (1/numsteps )\n",
    "lrvals = pd.Series([init_value*(mult**i) for i in  range(numsteps+ 1)])\n",
    "lrvals.plot(title='LR per step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossls = []\n",
    "optimizer.param_groups[0]['lr'] = init_value\n",
    "for t, (x, y) in enumerate(onecycleloader): \n",
    "    optimizer.zero_grad()\n",
    "    x = x.to(device)#.half()\n",
    "    y = y.cuda()\n",
    "    x = torch.autograd.Variable(x, requires_grad=True)#.half()\n",
    "    y = torch.autograd.Variable(y)\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.param_groups[0]['lr'] = init_value*(mult**t)\n",
    "    \n",
    "    ######One Cycle Policy##########>\n",
    "    #Compute the smoothed loss\n",
    "    batch_num += 1\n",
    "    avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "    smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "    lossls.append(smoothed_loss)\n",
    "\n",
    "    if t%20==0:\n",
    "        print('Step {} lr {:.6f} smoothed loss {:.5f} time {}'.format(t, init_value*(mult**t), smoothed_loss, datetime.now()))\n",
    "    del loss, out, y, x# , target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(lossls, index=np.log10(lrvals)).plot(title='Smoothed loss per LR (log10)', ylim=(6.9,7.3), figsize = (10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we train up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 3e-2\n",
    "model = DenseNet(num_classes=trndf.sirna.max()+1, num_channels = 6)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\", keep_batchnorm_fp32=False, loss_scale=\"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def prediction(model, loader):\n",
    "    probs = []\n",
    "    for x, _ in tqdm.tqdm(loader):\n",
    "        x = x.to(device)\n",
    "        output = model(x)\n",
    "        outmat = torch.sigmoid(output.cpu()).numpy()\n",
    "        probs.append(outmat)\n",
    "    probs = np.concatenate(probs, 0)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    tloss = 0.\n",
    "    model.train()\n",
    "    for t, (x, y) in enumerate(trnloader): \n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)#.half()\n",
    "        y = y.cuda()\n",
    "        x = torch.autograd.Variable(x, requires_grad=True)#.half()\n",
    "        y = torch.autograd.Variable(y)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        optimizer.step()\n",
    "        tloss += loss.item() \n",
    "        del loss, out, y, x\n",
    "    print('Epoch {} -> Train Loss: {:.4f} -> Time {}'.format(epoch+1, tloss/len(trnloader), datetime.now()))\n",
    "    \n",
    "    model.eval()\n",
    "    preds = prediction(model, valloader)\n",
    "    val_accuracy = (valdf.sirna.values == preds.argmax(1)).mean()\n",
    "    print('Epoch {} -> Val Acc: {:.4f} -> Time {}'.format(epoch+1, val_accuracy, datetime.now()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_file = \"recursion_model.bin\"\n",
    "torch.save(model.state_dict(), output_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now please implement test time augmentation to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load up the model\n",
    "# model.load_state_dict(torch.load(os.path.join(path_data, \"recursion_model.bin\")))\n",
    "# model.to(device)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# model.eval()\n",
    "# ...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
